{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2002504,"sourceType":"datasetVersion","datasetId":1198025}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Interpretable Depth Estimation Project\nBy Lien Huong and Evgeniia.\n\nThis work is based on the following papers:\n1. You, Z., Tsai, Y.-H., Chiu, W.-C., and Li, G. (2021). Towards Interpretable Deep Networks for Monocular Depth Estimation. arXiv.\n2. Papa, L., Russo, P., and Amerini, I. (2023). METER: A Mobile Vision Transformer Architecture for Monocular Depth Estimation. IEEE Transactions on Circuits and Systems for Video Technology, 33(10), 5882â€“5893.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nimport h5py\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport urllib.request\nfrom pathlib import Path\n\nimport gc\n\nfrom einops import rearrange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:30:49.590277Z","iopub.execute_input":"2025-07-19T20:30:49.590551Z","iopub.status.idle":"2025-07-19T20:30:49.595547Z","shell.execute_reply.started":"2025-07-19T20:30:49.590529Z","shell.execute_reply":"2025-07-19T20:30:49.594847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Globals","metadata":{}},{"cell_type":"code","source":"RGB_img_res = (None, 192, 256)\n\nneuron_selective = True # True: train with neuron selectivity; False: train with balanced loss function\n\n# Training configuration\nnum_epochs = 20\n# For train with neuron selectivity\nimde_alpha = -0.1\ncfg_layers = [\"dec_3\", \"enc_4\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:30:49.981162Z","iopub.execute_input":"2025-07-19T20:30:49.981404Z","iopub.status.idle":"2025-07-19T20:30:49.985243Z","shell.execute_reply.started":"2025-07-19T20:30:49.981388Z","shell.execute_reply":"2025-07-19T20:30:49.984427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # to avoid fragmentation\n\n# Set memory-efficient PyTorch settings\ntorch.backends.cudnn.benchmark = True\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory cache\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:30:50.178593Z","iopub.execute_input":"2025-07-19T20:30:50.178803Z","iopub.status.idle":"2025-07-19T20:30:50.244441Z","shell.execute_reply.started":"2025-07-19T20:30:50.178787Z","shell.execute_reply":"2025-07-19T20:30:50.243841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def get_num_units_bins_and_nums_for_plt(layers, default_num_bins=64):\n    num_units_layers_dict = {\n        \"enc_output\": 160,\n        \"enc_0\": 16, \"enc_1\": 16,\"enc_2\": 24, \"enc_3\": 24, \"enc_4\": 24,\"enc_5\": 48, # conv1, mv1-4\n        \"enc_6\": 48, \"enc_7\": 64, \"enc_8\": 64, \"enc_9\": 80, \"enc_10\": 80, # meter0, mv5, meter1, mv6, meter2\n        \"dec_0\": 64,\"dec_1\": 32,\"dec_2\": 16,\"dec_3\": 8 # conv1, upsampling1, upsampling2, upsampling3\n    }\n    num_units_list = [num_units_layers_dict[layer] for layer in layers]\n    num_bins_list = [min(num_units, default_num_bins) for num_units in num_units_list]\n\n    # for plotting\n    plt_row_list, plt_col_list, figsize_w_list, figsize_h_list = [], [], [], []\n    for num_units in num_units_list:\n        if num_units == 64:\n            plt_row, plt_col, figsize_h, figsize_w = 8, 8, 30, 50\n        elif num_units == 48:\n            plt_row, plt_col, figsize_h, figsize_w = 6, 8, 30, 50\n        elif num_units == 8:\n            plt_row, plt_col, figsize_h, figsize_w = 2, 4, 10, 20\n        elif num_units == 16:\n            plt_row, plt_col, figsize_h, figsize_w = 4, 4, 15, 20\n        elif num_units == 24:\n            plt_row, plt_col, figsize_h, figsize_w = 4, 6, 15, 30\n        elif num_units == 80:\n            plt_row, plt_col, figsize_h, figsize_w = 8, 10, 30, 60\n        else: # 160\n            plt_row, plt_col, figsize_h, figsize_w = 16, 10, 60, 50\n    \n        plt_row_list.append(plt_row)\n        plt_col_list.append(plt_col)\n        figsize_h_list.append(figsize_h)\n        figsize_w_list.append(figsize_w)\n\n    return num_units_list, num_bins_list, plt_row_list, plt_col_list, figsize_w_list, figsize_h_list\n\n\ndef get_sid_thresholds(K):\n    alpha, beta = 0.75, 100.\n\n    thresholds = [0., alpha]\n    for i in range(1, K-1):\n        ti = np.exp((np.log(beta + (1 - alpha)) * i) / (K-1)) - (1 - alpha)\n        thresholds.append(ti)\n    thresholds.append(beta)\n    return thresholds\n\n\ndef discretize_depths(depths, K=None, thresholds=None):\n    discrete_depths = torch.zeros(depths.shape).to(depths.device)\n    assert K or thresholds, \"At least one of them should be given.\"\n    if not thresholds:\n        thresholds = get_sid_thresholds(K)\n    else:\n        if not K:\n            K = len(thresholds) - 1\n        else:\n            assert K == len(thresholds) - 1\n    discrete_depths[torch.where(depths <= 0)] = -1  # invalid depth -> -1\n    for i in range(K):\n        discrete_depths[torch.where((depths > thresholds[i]) & (depths <= thresholds[i+1]))] = i\n    return discrete_depths\n\n\ndef count_discrete_depth(discrete_depth, discrete_depth_cnt=None):\n    device = torch.device(\"cpu\") if isinstance(discrete_depth, np.ndarray) else discrete_depth.device\n    if discrete_depth_cnt is None:\n        discrete_depth_cnt = torch.zeros(1, dtype=torch.double).to(device)\n    for img_i in range(len(discrete_depth)):\n        bin_idx_max = int(discrete_depth[img_i].max())\n        if bin_idx_max >= len(discrete_depth_cnt):  # need to extend discrete_depth_cnt\n            discrete_depth_cnt = torch.cat([discrete_depth_cnt, torch.zeros(bin_idx_max - len(discrete_depth_cnt) + 1, dtype=torch.double).to(device)], axis=-1)\n        for bin_idx_iter in range(bin_idx_max+1):\n            cur_xy_tuples = torch.where(discrete_depth[img_i][0] == bin_idx_iter)\n            if len(cur_xy_tuples[0]) == 0:\n                continue\n            discrete_depth_cnt[bin_idx_iter] += len(cur_xy_tuples[0])\n\n    return discrete_depth_cnt\n\n\ndef add_fmaps_on_discrete_depths(discrete_depths, fmaps, discrete_depth_sum=None):\n    device = fmaps.device\n    if discrete_depth_sum is None:\n        discrete_depth_sum = torch.zeros((fmaps.shape[1], 1), dtype=torch.double).to(device)\n    h, w = discrete_depths.shape[2], discrete_depths.shape[3]\n    fmaps = F.interpolate(fmaps, (h, w), mode='bilinear', align_corners=False)\n    for img_i in range(len(discrete_depths)):\n        bin_idx_max = int(discrete_depths[img_i].max())\n        if bin_idx_max >= discrete_depth_sum.shape[1]:     # need to extend discrete_depth_sum\n            discrete_depth_sum = torch.cat([discrete_depth_sum, torch.zeros((discrete_depth_sum.shape[0], bin_idx_max - discrete_depth_sum.shape[1] + 1), dtype=torch.double).to(device)], axis=-1)\n        for bin_idx_iter in range(bin_idx_max+1):\n            cur_xy_tuples = torch.where(discrete_depths[img_i][0] == bin_idx_iter)\n            if len(cur_xy_tuples[0]) == 0:\n                continue\n            discrete_depth_sum[:, bin_idx_iter] += (fmaps[img_i, :, cur_xy_tuples[0], cur_xy_tuples[1]]).sum(-1)\n\n    return discrete_depth_sum\n\n\ndef compute_selectivity(depth_avg_response, unit_max_bin):\n    if isinstance(depth_avg_response, np.ndarray):\n        depth_avg_response = torch.from_numpy(depth_avg_response)\n    response_max = depth_avg_response[np.arange(len(depth_avg_response)), unit_max_bin]\n    response_nonmax = (depth_avg_response.sum(1) - response_max) / (depth_avg_response.shape[1] - 1)\n    selectivity_index = (response_max - response_nonmax) / (response_max + response_nonmax + 1e-15)\n    return selectivity_index\n\n\ndef get_features(model, image, layer_list, also_get_output=True, is_training=False):\n    return_list = []\n\n    def forward(image, layer_list, also_get_output):\n        x, enc_layer = model.encoder(image)\n        output, dec_layer = model.decoder(x, enc_layer)\n    \n        for layer in layer_list:\n            if layer == 'enc_output':\n                return_list.append(x)\n            elif layer.startswith('enc_'):  # e.g., 'enc_0', 'enc_1', ..., 'enc_10'\n                idx = int(layer.split('_')[1])\n                return_list.append(enc_layer[idx])\n            elif layer.startswith('dec_'):  # e.g., 'dec_0', 'dec_1', ..., 'dec_3'\n                idx = int(layer.split('_')[1])\n                return_list.append(dec_layer[idx])\n            else:\n                raise NotImplementedError(f\"Layer {layer} is not supported.\")\n\n        if also_get_output:\n            return_list.append(output)\n\n        return return_list\n\n    if is_training:\n        return_list = forward(image, layer_list, also_get_output)\n    else:\n        with torch.no_grad():\n            return_list = forward(image, layer_list, also_get_output)\n\n    return return_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:01.010603Z","iopub.execute_input":"2025-07-19T20:31:01.011144Z","iopub.status.idle":"2025-07-19T20:31:01.029272Z","shell.execute_reply.started":"2025-07-19T20:31:01.011117Z","shell.execute_reply":"2025-07-19T20:31:01.028502Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dissection","metadata":{}},{"cell_type":"code","source":"def compute_and_plot_selectivity(model, layers, data_loader, viz_plot=True):\n    num_units_list, num_bins_list, plt_row_list, plt_col_list, figsize_w_list, figsize_h_list = get_num_units_bins_and_nums_for_plt(\n        layers=layers\n    )\n\n    depth_cnt_list, depth_sum_list, depth_avg_list = [], [], []\n    for layer_i in range(len(layers)):\n        depth_cnt_list.append(torch.zeros(num_bins_list[layer_i], dtype=torch.double).to(device))\n        depth_sum_list.append(torch.zeros((num_units_list[layer_i], num_bins_list[layer_i]), dtype=torch.double).to(device))\n    sid_thresholds_list = [get_sid_thresholds(num_bins_list[layer_i]) for layer_i in range(len(layers))]\n\n    for images, depths in tqdm(data_loader, desc='Dissection'):\n        imgs = images.to(device)\n        features_list = get_features(model, imgs, layers, also_get_output=False)\n        for layer_i in range(len(layers)):\n            depths = depths\n            discrete_concepts = discretize_depths(depths, thresholds=sid_thresholds_list[layer_i])\n            depth_cnt_list[layer_i] = count_discrete_depth(discrete_concepts, depth_cnt_list[layer_i])\n            depth_sum_list[layer_i] = add_fmaps_on_discrete_depths(discrete_concepts, features_list[layer_i],\n                                                                   depth_sum_list[layer_i])\n\n    for layer_i in range(len(layers)):\n        depth_cnt_list[layer_i] = depth_cnt_list[layer_i].cpu().numpy()\n        depth_sum_list[layer_i] = depth_sum_list[layer_i].cpu().numpy()\n        depth_avg_list.append(depth_sum_list[layer_i] / (depth_cnt_list[layer_i] + 1e-15))\n\n    unit_max_bin_list, selectivity_index_list = [], []\n    for layer_i in range(len(layers)):\n        depth_avg = depth_avg_list[layer_i]\n        # ---- compute selectivity ----\n        depth_avg_abs = abs(depth_avg)\n        unit_max_bin = depth_avg_abs.argmax(-1)\n        selectivity_index = compute_selectivity(depth_avg_abs, unit_max_bin).numpy()\n        unit_max_bin_list.append(unit_max_bin)\n        selectivity_index_list.append(selectivity_index)\n        selec_mean = selectivity_index.mean()\n        print(f'selec_mean_{layers[layer_i]}', selectivity_index.mean())\n        # ---- plot ----\n        if viz_plot:\n            sns.set_theme(style=\"darkgrid\")\n            fig, ax = plt.subplots(plt_row_list[layer_i], plt_col_list[layer_i],\n                                   figsize=(figsize_w_list[layer_i], figsize_h_list[layer_i]))\n        \n            for i in range(plt_row_list[layer_i]):\n                for j in range(plt_col_list[layer_i]):\n                    unit_i = i * plt_col_list[layer_i] + j\n                    ax[i, j].set_title(\n                        f'Unit {unit_i}, Layer {layers[layer_i]}, Selectivity: {selectivity_index[unit_i]:.04f}',\n                        fontsize=12)\n                    ax[i, j].bar(\n                        range(len(depth_cnt_list[layer_i])),\n                        depth_avg_abs[unit_i], # using abs value here (originally depth_avg)\n                        color='red', width=0.85)\n                    plt.xticks(fontsize=12)\n                    plt.yticks(fontsize=12)\n                    plt.subplots_adjust(wspace=0.15)\n            save_fig_name = f'{layers[layer_i]}-{selec_mean:.4f}.png'\n            fig.savefig(save_fig_name, bbox_inches='tight')\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:10.117487Z","iopub.execute_input":"2025-07-19T20:31:10.118085Z","iopub.status.idle":"2025-07-19T20:31:10.127807Z","shell.execute_reply.started":"2025-07-19T20:31:10.118059Z","shell.execute_reply":"2025-07-19T20:31:10.127028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Metric","metadata":{}},{"cell_type":"code","source":"def compute_rmse(outputs, depths):\n    mse = torch.mean((outputs - depths) ** 2)\n    rmse = torch.sqrt(mse)\n    return rmse.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:10.739742Z","iopub.execute_input":"2025-07-19T20:31:10.740313Z","iopub.status.idle":"2025-07-19T20:31:10.743870Z","shell.execute_reply.started":"2025-07-19T20:31:10.740286Z","shell.execute_reply":"2025-07-19T20:31:10.743195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"def visualize_predictions(model, train_loader, val_loader, device, epoch, num_samples=2, save=False):\n    \"\"\"Visualize predictions on training and validation data\"\"\"\n    model.eval()\n    \n    # Get batches of training and validation data\n    train_images, train_depths = next(iter(train_loader))\n    val_images, val_depths = next(iter(val_loader))\n    \n    # Generate predictions\n    with torch.no_grad():\n        train_predictions = model(train_images.to(device))\n        val_predictions = model(val_images.to(device))\n    \n    # Convert tensors to numpy arrays\n    train_images = train_images.cpu().numpy()\n    train_depths = train_depths.cpu().numpy()\n    train_predictions = train_predictions.cpu().numpy()\n    \n    val_images = val_images.cpu().numpy()\n    val_depths = val_depths.cpu().numpy()\n    val_predictions = val_predictions.cpu().numpy()\n    \n    # Create figure\n    fig, axes = plt.subplots(2*num_samples, 3, figsize=(15, 5*2*num_samples))\n    plt.suptitle(f'Training and Validation Results - Epoch {epoch+1}', fontsize=16)\n    \n    def plot_sample(img, depth_gt, depth_pred, row_idx, title_prefix):\n        # Original image\n        img_display = np.transpose(img, (1, 2, 0))\n        img_display = (img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n        img_display = img_display.astype(np.uint8)\n        axes[row_idx, 0].imshow(img_display)\n        axes[row_idx, 0].set_title(f'{title_prefix} Input Image')\n        axes[row_idx, 0].axis('off')\n        \n        # Ground truth depth\n        axes[row_idx, 1].imshow(depth_gt[0], cmap='plasma')\n        axes[row_idx, 1].set_title(f'{title_prefix} Ground Truth Depth')\n        axes[row_idx, 1].axis('off')\n        \n        # Predicted depth\n        axes[row_idx, 2].imshow(depth_pred[0], cmap='plasma')\n        axes[row_idx, 2].set_title(f'{title_prefix} Predicted Depth')\n        axes[row_idx, 2].axis('off')\n    \n    # Plot training samples\n    for i in range(num_samples):\n        plot_sample(train_images[i], train_depths[i], train_predictions[i], 2*i, 'Train:')\n    \n    # Plot validation samples\n    for i in range(num_samples):\n        plot_sample(val_images[i], val_depths[i], val_predictions[i], 2*i + 1, 'Val:')\n    \n    plt.tight_layout()\n    if save:\n        os.makedirs('training_visualizations', exist_ok=True)\n        plt.savefig(f'training_visualizations/epoch_{epoch+1}.png')\n        # plt.close()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:11.278325Z","iopub.execute_input":"2025-07-19T20:31:11.278566Z","iopub.status.idle":"2025-07-19T20:31:11.287311Z","shell.execute_reply.started":"2025-07-19T20:31:11.278549Z","shell.execute_reply":"2025-07-19T20:31:11.286596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data: NYUDepthV2\nhttps://www.kaggle.com/datasets/soumikrakshit/nyu-depth-v2","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_WORKERS = 2\n\ntrain_csv_path = '/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv'\nbase_data_path = '/kaggle/input/nyu-depth-v2/nyu_data'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:11.933012Z","iopub.execute_input":"2025-07-19T20:31:11.933695Z","iopub.status.idle":"2025-07-19T20:31:11.937066Z","shell.execute_reply.started":"2025-07-19T20:31:11.933670Z","shell.execute_reply":"2025-07-19T20:31:11.936323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NYUDepthV2Dataset(Dataset):\n    def __init__(self, filenames_df, transform=None):\n        \"\"\"\n        Args:\n            filenames_df: DataFrame containing image and depth map paths\n            transform: Optional transform to be applied on the images\n        \"\"\"\n        self.filenames_df = filenames_df\n        self.transform = transform\n        # Define resolutions as constants from the paper\n        self.RGB_SIZE = (192, 256)  # (H, W) for input\n        self.DEPTH_SIZE = (48, 64)  # (H, W) for depth maps\n\n    def __len__(self):\n        return len(self.filenames_df)\n\n    def __getitem__(self, idx):\n        # Get image and depth paths\n        img_path = self.filenames_df.iloc[idx, 0]\n        depth_path = self.filenames_df.iloc[idx, 1]\n        \n        # Load image and depth map\n        image = Image.open(img_path).convert('RGB')\n        depth = Image.open(depth_path)\n        \n        # Resize to the specified resolutions\n        image = image.resize((self.RGB_SIZE[1], self.RGB_SIZE[0]), Image.BILINEAR)  # PIL uses (W, H)\n        depth = depth.resize((self.DEPTH_SIZE[1], self.DEPTH_SIZE[0]), Image.NEAREST)  # PIL uses (W, H)\n        \n        # Convert to tensors\n        if self.transform:\n            image = self.transform(image)\n        depth = torch.from_numpy(np.array(depth)).float().unsqueeze(0)  # Add channel dimension\n        \n        return image, depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:12.339905Z","iopub.execute_input":"2025-07-19T20:31:12.340552Z","iopub.status.idle":"2025-07-19T20:31:12.346675Z","shell.execute_reply.started":"2025-07-19T20:31:12.340529Z","shell.execute_reply":"2025-07-19T20:31:12.345812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataset_loaders(train_csv_path, base_data_path, batch_size, num_workers):\n    train_csv = Path(train_csv_path)\n    base_path = Path(base_data_path)\n\n    # Load the dataset paths\n    filenames_df = pd.read_csv(train_csv, header=None)\n    filenames_df[0] = filenames_df[0].map(lambda x: base_path/x)\n    filenames_df[1] = filenames_df[1].map(lambda x: base_path/x)\n\n    # Define transforms\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Split dataset into train and validation sets\n    total_size = len(filenames_df)\n    train_size = int(0.8 * total_size)\n    val_size = int(0.1 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    train_df = filenames_df[:train_size]\n    val_df = filenames_df[train_size:train_size + val_size]\n    test_df = filenames_df[train_size + val_size:]\n    \n    print(f\"Dataset splits: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        NYUDepthV2Dataset(train_df, transform), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    val_loader = DataLoader(\n        NYUDepthV2Dataset(val_df, transform), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    test_loader = DataLoader(\n        NYUDepthV2Dataset(test_df, transform), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:12.708836Z","iopub.execute_input":"2025-07-19T20:31:12.709342Z","iopub.status.idle":"2025-07-19T20:31:12.715867Z","shell.execute_reply.started":"2025-07-19T20:31:12.709316Z","shell.execute_reply":"2025-07-19T20:31:12.715133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_dataset_loaders(\n    train_csv_path,\n    base_data_path,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:12.989136Z","iopub.execute_input":"2025-07-19T20:31:12.989848Z","iopub.status.idle":"2025-07-19T20:31:13.602368Z","shell.execute_reply.started":"2025-07-19T20:31:12.989821Z","shell.execute_reply":"2025-07-19T20:31:13.601765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Network: METER model (XXS)","metadata":{}},{"cell_type":"code","source":"class SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n                                   kernel_size=kernel_size,\n                                   groups=depth,\n                                   padding=1,\n                                   stride=stride,\n                                   bias=bias).to(device)\n        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU()\n    )\n\ndef conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n    return nn.Sequential(\n        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n                        bias=False, device='cuda:0'),\n        nn.BatchNorm2d(oup),\n        nn.ReLU()\n    )\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim=-1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        attn = self.attend(dots)\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b p h n d -> b p n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass MV2Block(nn.Module):\n    def __init__(self, inp, oup, stride=1, expansion=4):\n        super().__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(inp * expansion)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        \n        self.conv = nn.Sequential(\n            nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(),\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(),\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\nclass MobileViTBlock(nn.Module):\n    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n        super().__init__()\n        self.ph, self.pw = patch_size\n\n        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n        self.conv2 = conv_1x1_bn(channel, dim)\n\n        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n\n        self.conv3 = conv_1x1_bn(dim, channel)\n        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n\n    def forward(self, x):\n        y = x.clone()\n\n        # Local representations\n        x = self.conv1(x)\n        x = self.conv2(x)\n\n        # Global representations\n        _, _, h, w = x.shape\n        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n        x = self.transformer(x)\n        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n                      pw=self.pw)\n\n        # Fusion\n        x = self.conv3(x)\n        x = torch.cat((x, y), 1)\n        x = self.conv4(x)\n        return x\n\n\nclass MobileViT(nn.Module):\n    def __init__(self, image_size, dims, channels, expansion=4, kernel_size=3, patch_size=(2, 2)):\n        super().__init__()\n        ih, iw = image_size\n        ph, pw = patch_size\n        assert ih % ph == 0 and iw % pw == 0\n\n        L = [1, 1, 1]\n\n        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n\n        self.mv2 = nn.ModuleList([])\n        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n\n        self.mvit = nn.ModuleList([])\n        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n\n        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n\n    def forward(self, x):\n        econv1 = self.conv1(x)\n        mv0 = self.mv2[0](econv1)\n\n        mv1 = self.mv2[1](mv0)\n        mv2 = self.mv2[2](mv1)\n        mv3 = self.mv2[3](mv2)\n\n        mv4 = self.mv2[4](mv3)\n        meter0 = self.mvit[0](mv4)\n\n        mv5 = self.mv2[5](meter0)\n        meter1 = self.mvit[1](mv5)\n\n        mv6 = self.mv2[6](meter1)\n        meter2 = self.mvit[2](mv6)\n        x = self.conv2(meter2)\n        return x, [econv1, mv0, mv1, mv2, mv3, mv4, meter0, mv5, meter1, mv6, meter2]\n\ndef mobilevit_xxs():\n    enc_type = 'xxs'\n    dims = [64, 80, 96]\n    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]\n    return MobileViT((RGB_img_res[1], RGB_img_res[2]), dims, channels, expansion=2)\n\nclass UpSample_layer(nn.Module):\n    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n        super(UpSample_layer, self).__init__()\n        self.flag = flag\n        self.name = name\n        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n                                                   dilation=1, output_padding=(1, 1), bias=False)\n        self.end_up_layer = nn.Sequential(\n            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n            nn.ReLU()\n        )\n\n    def forward(self, x, enc_layer):\n        x = self.conv2d_transpose(x)\n        if x.shape[-1] != enc_layer.shape[-1]:\n            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n        if x.shape[-1] != enc_layer.shape[-1]:\n            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n        x = torch.cat([x, enc_layer], dim=1)\n        x = self.end_up_layer(x)\n        return x\n\nclass decoder(nn.Module):\n    def __init__(self, device):\n        super(decoder, self).__init__()\n        self.conv2d_in = nn.Conv2d(160, 64, kernel_size=(1, 1), padding='same', bias=False)\n        self.ups_block_1 = UpSample_layer(64, 32, flag=True, sep_conv_filters=96, name='up1', device=device)\n        self.ups_block_2 = UpSample_layer(32, 16, flag=False, sep_conv_filters=64, name='up2', device=device)\n        self.ups_block_3 = UpSample_layer(16, 8, flag=False, sep_conv_filters=32, name='up3', device=device)\n        self.conv2d_out = nn.Conv2d(8, 1, kernel_size=(3, 3), padding='same', bias=False)\n\n    def forward(self, x, enc_layer_list):\n        dconv0 = self.conv2d_in(x)\n        up1 = self.ups_block_1(dconv0, enc_layer_list[7]) #enc_layer_list[3]\n        up2 = self.ups_block_2(up1, enc_layer_list[5]) #enc_layer_list[2]\n        up3 = self.ups_block_3(up2, enc_layer_list[2]) #enc_layer_list[1]\n        x = self.conv2d_out(up3)\n        return x, [dconv0, up1, up2, up3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:13.721212Z","iopub.execute_input":"2025-07-19T20:31:13.721642Z","iopub.status.idle":"2025-07-19T20:31:13.750647Z","shell.execute_reply.started":"2025-07-19T20:31:13.721625Z","shell.execute_reply":"2025-07-19T20:31:13.750071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class build_METER_model(nn.Module):\n    def __init__(self, device):\n        super(build_METER_model, self).__init__()\n        self.encoder = mobilevit_xxs()\n        self.decoder = decoder(device=device)\n\n    def forward(self, x):\n        x, enc_layer = self.encoder(x)\n        x, dec_layer = self.decoder(x, enc_layer)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:14.082452Z","iopub.execute_input":"2025-07-19T20:31:14.082683Z","iopub.status.idle":"2025-07-19T20:31:14.087072Z","shell.execute_reply.started":"2025-07-19T20:31:14.082664Z","shell.execute_reply":"2025-07-19T20:31:14.086344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train\n### Baseline loss function","metadata":{}},{"cell_type":"code","source":"def gaussian(window_size, sigma):\n    gauss = torch.Tensor([np.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\n\ndef create_window(window_size, channel=1):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n    return window\n\n\ndef ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n    if val_range is None:\n        if torch.max(img1) > 128:\n            max_val = 255\n        else:\n            max_val = 1\n\n        if torch.min(img1) < -0.5:\n            min_val = -1\n        else:\n            min_val = 0\n        L = max_val - min_val\n    else:\n        L = val_range\n\n    padd = 0\n    (_, channel, height, width) = img1.size()\n    if window is None:\n        real_size = min(window_size, height, width)\n        window = create_window(real_size, channel=channel).to(img1.device)\n\n    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n\n    C1 = (0.01 * L) ** 2\n    C2 = (0.03 * L) ** 2\n\n    v1 = 2.0 * sigma12 + C2\n    v2 = sigma1_sq + sigma2_sq + C2\n    cs = torch.mean(v1 / v2)  # contrast sensitivity\n\n    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n\n    if size_average:\n        ret = ssim_map.mean()\n    else:\n        ret = ssim_map.mean(1).mean(1).mean(1)\n\n    if full:\n        return ret, cs\n    return ret\n\n\nclass Sobel(nn.Module):\n    def __init__(self):\n        super(Sobel, self).__init__()\n        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n        edge_k = np.stack((edge_kx, edge_ky))\n\n        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n        self.edge_conv.weight = nn.Parameter(edge_k)\n\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        out = self.edge_conv(x)\n        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:14.668634Z","iopub.execute_input":"2025-07-19T20:31:14.668898Z","iopub.status.idle":"2025-07-19T20:31:14.680232Z","shell.execute_reply.started":"2025-07-19T20:31:14.668878Z","shell.execute_reply":"2025-07-19T20:31:14.679545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class balanced_loss_function(nn.Module):\n    def __init__(self, device):\n        super(balanced_loss_function, self).__init__()\n        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n        self.get_gradient = Sobel().to(device)\n        self.device = device\n        self.lambda_1 = 0.5\n        self.lambda_2 = 100\n        self.lambda_3 = 100\n\n    def forward(self, output, depth):\n        with torch.no_grad():\n            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n\n        depth_grad = self.get_gradient(depth)\n        output_grad = self.get_gradient(output)\n\n        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n\n        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n\n        loss_depth = torch.abs(output - depth).mean()\n        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n        loss_normal = self.lambda_2 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n\n        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * self.lambda_3\n\n        loss_grad = (loss_dx + loss_dy) / self.lambda_1\n\n        loss = loss_depth + loss_ssim + loss_normal + loss_grad\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:14.995664Z","iopub.execute_input":"2025-07-19T20:31:14.996131Z","iopub.status.idle":"2025-07-19T20:31:15.003208Z","shell.execute_reply.started":"2025-07-19T20:31:14.996107Z","shell.execute_reply":"2025-07-19T20:31:15.002531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Neuron selectivity regularizer","metadata":{}},{"cell_type":"code","source":"def get_loss_with_regularizer(layers, model, imgs, depths, sid_thresholds_list, device, criterion):\n    features_list = get_features(model, imgs, layers, also_get_output=True, is_training=True)\n    preds = features_list[-1]\n    baseline_loss = criterion(preds, depths)\n\n    # ---- selectivity regularizer ----\n    selec_list = []\n    for layer_i in range(len(layers)):\n        discret_depths = discretize_depths(depths, thresholds=sid_thresholds_list[layer_i])\n        concept_cnt = count_discrete_depth(discret_depths, torch.zeros(len(sid_thresholds_list[layer_i])-1, dtype=torch.double).to(device))\n        features_list[layer_i] = torch.abs(features_list[layer_i])\n        concept_sum = add_fmaps_on_discrete_depths(discret_depths, features_list[layer_i], torch.zeros((features_list[layer_i].shape[1], len(sid_thresholds_list[layer_i])-1), dtype=torch.double).to(device))\n        concept_avg = concept_sum / (concept_cnt + 1e-15)\n\n        num_units, num_bins = concept_avg.shape[0], concept_avg.shape[1]\n        assert num_units >= num_bins\n        unit_class = (torch.arange(num_units)//(num_units/num_bins)).to(concept_avg.device).long()\n        layer_selec = compute_selectivity(concept_avg, unit_class)\n        concept_cnt_iszero = (unit_class == torch.nonzero(concept_cnt==0)).sum(0, dtype=torch.bool)\n        layer_selec[concept_cnt_iszero] = 0   # If d_k is absent in a batch, the unit k will be simply disregarded from the loss computation\n        layer_selec = layer_selec.mean()\n        selec_list.append(layer_selec)\n    selectivity = sum(selec_list)\n    return baseline_loss, selectivity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:15.747074Z","iopub.execute_input":"2025-07-19T20:31:15.747624Z","iopub.status.idle":"2025-07-19T20:31:15.754110Z","shell.execute_reply.started":"2025-07-19T20:31:15.747604Z","shell.execute_reply":"2025-07-19T20:31:15.753544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"# Training configuration\nbest_val_loss = float('inf')\nmodel = build_METER_model(device).to(device)\ncriterion = balanced_loss_function(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n\n# For train with neuron selectivity\nsid_thresholds_list = []\nnum_bins_list = get_num_units_bins_and_nums_for_plt(cfg_layers)[1]\nfor layer_i in range(len(cfg_layers)):\n    sid_thresholds_list.append(get_sid_thresholds(num_bins_list[layer_i]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:16.366827Z","iopub.execute_input":"2025-07-19T20:31:16.367389Z","iopub.status.idle":"2025-07-19T20:31:16.569406Z","shell.execute_reply.started":"2025-07-19T20:31:16.367365Z","shell.execute_reply":"2025-07-19T20:31:16.568844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss, selectivity_sum, baseline_loss_sum = 0, 0, 0\n    for images, depths in tqdm(train_loader, desc='Training'):\n        images = images.to(device)\n        depths = depths.to(device)\n\n        loss = None\n        if neuron_selective:\n            baseline_loss, selectivity = get_loss_with_regularizer(cfg_layers, model, images, depths, sid_thresholds_list, device, criterion)\n            loss = baseline_loss + imde_alpha * selectivity\n            selectivity_sum += selectivity.item()\n            baseline_loss_sum += baseline_loss.item()\n        else:\n            outputs = model(images)\n            loss = criterion(outputs, depths)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss/len(train_loader), selectivity_sum/len(train_loader)\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    rmse = 0\n    \n    with torch.no_grad():\n        for images, depths in tqdm(val_loader, desc='Validating'):\n            images = images.to(device)\n            depths = depths.to(device)\n            \n            outputs = model(images)\n            loss = None\n            if neuron_selective:\n                baseline_loss, selectivity = get_loss_with_regularizer(cfg_layers, model, images, depths, sid_thresholds_list, device, criterion)\n                loss = baseline_loss + imde_alpha * selectivity\n            else:\n                loss = criterion(outputs, depths)\n            \n            total_loss += loss.item()\n            rmse += compute_rmse(outputs, depths)\n\n    return total_loss / len(val_loader), rmse / len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:17.173465Z","iopub.execute_input":"2025-07-19T20:31:17.174004Z","iopub.status.idle":"2025-07-19T20:31:17.181449Z","shell.execute_reply.started":"2025-07-19T20:31:17.173980Z","shell.execute_reply":"2025-07-19T20:31:17.180697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\ntrain_losses, val_losses, selectivities = [], [], []\nrmse_values = []\nprint(f'***** Neuron selectivite: {neuron_selective} *****')\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    \n    # Train\n    train_loss, selectivity = train_one_epoch(model, train_loader, criterion, optimizer, device)\n    train_losses.append(train_loss)\n    selectivities.append(selectivity)\n    print(f'Training Loss: {train_loss:.4f}')\n    if neuron_selective:\n        print(f'Selectivity: {selectivity:.4f}')\n    \n    # Validate\n    val_loss, rmse = validate(model, val_loader, criterion, device)\n    val_losses.append(val_loss)\n    rmse_values.append(rmse)\n    print(f'Validation Loss: {val_loss:.4f} RMSE: {rmse:.4f}')\n    \n    # Visualize predictions on training and validation data\n    visualize_predictions(model, train_loader, val_loader, device, epoch, save=True)\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'selectivity': selectivity,\n            'val_loss': val_loss,\n            'rmse': rmse,\n        }, 'best_meter_model.pth')\n        print(f'best rmse: {rmse}')\n    print('-' * 50)\n    clear_gpu_memory()\n# Save logs\nnp.save('train_losses.npy', train_losses)\nnp.save('val_losses.npy', val_losses)\nnp.save('selectivities.npy', selectivities)\nnp.save('rmse_values.npy', rmse_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T20:22:22.293948Z","iopub.execute_input":"2025-07-17T20:22:22.294565Z","iopub.status.idle":"2025-07-17T20:22:22.298579Z","shell.execute_reply.started":"2025-07-17T20:22:22.294543Z","shell.execute_reply":"2025-07-17T20:22:22.297717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plots","metadata":{}},{"cell_type":"code","source":"# Load logs\n# train_losses = np.load('train_losses.npy')\n# val_losses = np.load('val_losses.npy')\n# selectivities = np.load('selectivities.npy')\n# rmse_values = np.load('rmse_values.npy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T20:26:30.699180Z","iopub.execute_input":"2025-07-17T20:26:30.699460Z","iopub.status.idle":"2025-07-17T20:26:30.704954Z","shell.execute_reply.started":"2025-07-17T20:26:30.699438Z","shell.execute_reply":"2025-07-17T20:26:30.704374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot train & val losses\nplt_epochs = range(1, num_epochs+1)\nplt.plot(plt_epochs, train_losses, label='Training Loss')\nplt.plot(plt_epochs, val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(np.arange(0, num_epochs+1, 2))\nplt.legend(loc='best')\nplt.savefig(f'train_val_loss.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T20:26:32.846346Z","iopub.execute_input":"2025-07-17T20:26:32.847047Z","iopub.status.idle":"2025-07-17T20:26:33.202467Z","shell.execute_reply.started":"2025-07-17T20:26:32.847019Z","shell.execute_reply":"2025-07-17T20:26:33.201665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot RMSE\nplt_epochs = range(1, num_epochs+1)\nplt.plot(plt_epochs, rmse_values, label='RMSE')\nplt.title('Root mean squared error')\nplt.xlabel('Epochs')\nplt.ylabel('RMSE')\nplt.xticks(np.arange(0, num_epochs+1, 2))\nplt.legend(loc='best')\nplt.savefig(f'rmse.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T20:26:37.428036Z","iopub.execute_input":"2025-07-17T20:26:37.428316Z","iopub.status.idle":"2025-07-17T20:26:37.754420Z","shell.execute_reply.started":"2025-07-17T20:26:37.428297Z","shell.execute_reply":"2025-07-17T20:26:37.753668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot selectivity\nplt_epochs = range(1, num_epochs+1)\nplt.plot(plt_epochs, selectivities, label='Selectivity')\nplt.title('Neuron depth selectivity')\nplt.xlabel('Epochs')\nplt.ylabel('Selectivity')\nplt.xticks(np.arange(0, num_epochs+1, 2))\nplt.legend(loc='best')\nplt.savefig(f'selectivity.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T20:28:01.073080Z","iopub.execute_input":"2025-07-17T20:28:01.073659Z","iopub.status.idle":"2025-07-17T20:28:01.424011Z","shell.execute_reply.started":"2025-07-17T20:28:01.073636Z","shell.execute_reply":"2025-07-17T20:28:01.423200Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# !gdown --folder https://drive.google.com/drive/folders/1uz1VgAqYP17idqKvXAKiFTJpoPXDYQ5m -O model\n# !ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:52:28.099225Z","iopub.execute_input":"2025-07-19T20:52:28.099536Z","iopub.status.idle":"2025-07-19T20:52:46.489418Z","shell.execute_reply.started":"2025-07-19T20:52:28.099511Z","shell.execute_reply":"2025-07-19T20:52:46.488450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = build_METER_model(device).to(device)\nmodel.load_state_dict(torch.load('best_meter_model.pth')['model_state_dict'])\n# If loading models from gDrive\n# model.load_state_dict(torch.load('model/best_meter_model-2.pth')['model_state_dict'])\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:50.597396Z","iopub.execute_input":"2025-07-19T20:31:50.597664Z","iopub.status.idle":"2025-07-19T20:31:50.757538Z","shell.execute_reply.started":"2025-07-19T20:31:50.597640Z","shell.execute_reply":"2025-07-19T20:31:50.756836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layers_to_compute_selectivity = [\"enc_0\", \"enc_1\", \"enc_2\",\"enc_3\",\"enc_4\", \"enc_5\", \"enc_6\",\"enc_7\",\"enc_8\", \"enc_9\", \"enc_10\", \"enc_output\"]\ncompute_and_plot_selectivity(model, layers=layers_to_compute_selectivity, data_loader=val_loader, viz_plot=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:31:55.631743Z","iopub.execute_input":"2025-07-19T20:31:55.632025Z","iopub.status.idle":"2025-07-19T20:36:52.215524Z","shell.execute_reply.started":"2025-07-19T20:31:55.632004Z","shell.execute_reply":"2025-07-19T20:36:52.214374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layers_to_compute_selectivity = [\"dec_0\", \"dec_1\", \"dec_2\",\"dec_3\"]\ncompute_and_plot_selectivity(model, layers=layers_to_compute_selectivity, data_loader=val_loader, viz_plot=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:36:52.217304Z","iopub.execute_input":"2025-07-19T20:36:52.217623Z","iopub.status.idle":"2025-07-19T20:38:04.479346Z","shell.execute_reply.started":"2025-07-19T20:36:52.217593Z","shell.execute_reply":"2025-07-19T20:38:04.478515Z"}},"outputs":[],"execution_count":null}]}