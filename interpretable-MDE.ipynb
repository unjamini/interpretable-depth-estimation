{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Depth Estimation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-19T13:38:16.715058Z",
     "iopub.status.busy": "2025-07-19T13:38:16.714759Z",
     "iopub.status.idle": "2025-07-19T13:38:16.720095Z",
     "shell.execute_reply": "2025-07-19T13:38:16.719348Z",
     "shell.execute_reply.started": "2025-07-19T13:38:16.715036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import gc\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:38:18.360663Z",
     "iopub.status.busy": "2025-07-19T13:38:18.360191Z",
     "iopub.status.idle": "2025-07-19T13:38:18.364168Z",
     "shell.execute_reply": "2025-07-19T13:38:18.363427Z",
     "shell.execute_reply.started": "2025-07-19T13:38:18.360639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RGB_img_res = (None, 192, 256)\n",
    "neuron_selective = True # True: train with neuron selectivity; False: train with balanced loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:38:18.571520Z",
     "iopub.status.busy": "2025-07-19T13:38:18.570984Z",
     "iopub.status.idle": "2025-07-19T13:38:18.635266Z",
     "shell.execute_reply": "2025-07-19T13:38:18.634496Z",
     "shell.execute_reply.started": "2025-07-19T13:38:18.571494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  # to avoid fragmentation\n",
    "\n",
    "# Set memory-efficient PyTorch settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:20.979882Z",
     "iopub.status.busy": "2025-07-19T14:07:20.979594Z",
     "iopub.status.idle": "2025-07-19T14:07:21.000030Z",
     "shell.execute_reply": "2025-07-19T14:07:20.999272Z",
     "shell.execute_reply.started": "2025-07-19T14:07:20.979857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_num_units_bins_and_nums_for_plt(layers, default_num_bins=64):\n",
    "    num_units_layers_dict = {\n",
    "        \"enc_output\": 160,\n",
    "        # \"enc_0\": 16, \"enc_1\": 24, \"enc_2\": 48, \"enc_3\": 64,\n",
    "        \"enc_0\": 16,\"enc_1\": 24, \"enc_2\": 24, \"enc_3\": 24,\"enc_4\": 48, # conv1, mv1-4\n",
    "        \"enc_5\": 48, \"enc_6\": 64, \"enc_7\": 64, \"enc_8\": 80, \"enc_9\": 80, # meter0, mv5, meter1, mv6, meter2\n",
    "        \"dec_0\": 64,\"dec_1\": 32,\"dec_2\": 16,\"dec_3\": 8\n",
    "    }\n",
    "    num_units_list = [num_units_layers_dict[layer] for layer in layers]\n",
    "    num_bins_list = [min(num_units, default_num_bins) for num_units in num_units_list]\n",
    "\n",
    "    # for plotting\n",
    "    plt_row_list, plt_col_list, figsize_w_list, figsize_h_list = [], [], [], []\n",
    "    for num_units in num_units_list:\n",
    "        if num_units == 64:\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 8, 8, 30, 50\n",
    "        elif num_units == 48:\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 6, 8, 30, 50\n",
    "        elif num_units == 8:\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 2, 4, 10, 20\n",
    "        elif num_units == 16:\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 4, 4, 15, 20\n",
    "        elif num_units == 24:\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 4, 6, 15, 30\n",
    "        elif num_units == 80:\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 8, 10, 30, 60\n",
    "        else: # 160\n",
    "            plt_row, plt_col, figsize_h, figsize_w = 16, 10, 60, 50\n",
    "    \n",
    "        plt_row_list.append(plt_row)\n",
    "        plt_col_list.append(plt_col)\n",
    "        figsize_h_list.append(figsize_h)\n",
    "        figsize_w_list.append(figsize_w)\n",
    "\n",
    "    return num_units_list, num_bins_list, plt_row_list, plt_col_list, figsize_w_list, figsize_h_list\n",
    "\n",
    "\n",
    "def get_sid_thresholds(K):\n",
    "    alpha, beta = 0.75, 100.\n",
    "\n",
    "    thresholds = [0., alpha]\n",
    "    for i in range(1, K-1):\n",
    "        ti = np.exp((np.log(beta + (1 - alpha)) * i) / (K-1)) - (1 - alpha)\n",
    "        thresholds.append(ti)\n",
    "    thresholds.append(beta)\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def discretize_depths(depths, K=None, thresholds=None):\n",
    "    discrete_depths = torch.zeros(depths.shape).to(depths.device)\n",
    "    assert K or thresholds, \"At least one of them should be given.\"\n",
    "    if not thresholds:\n",
    "        thresholds = get_sid_thresholds(K)\n",
    "    else:\n",
    "        if not K:\n",
    "            K = len(thresholds) - 1\n",
    "        else:\n",
    "            assert K == len(thresholds) - 1\n",
    "    discrete_depths[torch.where(depths <= 0)] = -1  # invalid depth -> -1\n",
    "    for i in range(K):\n",
    "        discrete_depths[torch.where((depths > thresholds[i]) & (depths <= thresholds[i+1]))] = i\n",
    "    return discrete_depths\n",
    "\n",
    "\n",
    "def count_discrete_depth(discrete_depth, discrete_depth_cnt=None):\n",
    "    device = torch.device(\"cpu\") if isinstance(discrete_depth, np.ndarray) else discrete_depth.device\n",
    "    if discrete_depth_cnt is None:\n",
    "        discrete_depth_cnt = torch.zeros(1, dtype=torch.double).to(device)\n",
    "    for img_i in range(len(discrete_depth)):\n",
    "        bin_idx_max = int(discrete_depth[img_i].max())\n",
    "        if bin_idx_max >= len(discrete_depth_cnt):  # need to extend discrete_depth_cnt\n",
    "            discrete_depth_cnt = torch.cat([discrete_depth_cnt, torch.zeros(bin_idx_max - len(discrete_depth_cnt) + 1, dtype=torch.double).to(device)], axis=-1)\n",
    "        for bin_idx_iter in range(bin_idx_max+1):\n",
    "            cur_xy_tuples = torch.where(discrete_depth[img_i][0] == bin_idx_iter)\n",
    "            if len(cur_xy_tuples[0]) == 0:\n",
    "                continue\n",
    "            discrete_depth_cnt[bin_idx_iter] += len(cur_xy_tuples[0])\n",
    "\n",
    "    return discrete_depth_cnt\n",
    "\n",
    "\n",
    "def add_fmaps_on_discrete_depths(discrete_depths, fmaps, discrete_depth_sum=None):\n",
    "    device = fmaps.device\n",
    "    if discrete_depth_sum is None:\n",
    "        discrete_depth_sum = torch.zeros((fmaps.shape[1], 1), dtype=torch.double).to(device)\n",
    "    h, w = discrete_depths.shape[2], discrete_depths.shape[3]\n",
    "    fmaps = F.interpolate(fmaps, (h, w), mode='bilinear', align_corners=False)\n",
    "    for img_i in range(len(discrete_depths)):\n",
    "        bin_idx_max = int(discrete_depths[img_i].max())\n",
    "        if bin_idx_max >= discrete_depth_sum.shape[1]:     # need to extend discrete_depth_sum\n",
    "            discrete_depth_sum = torch.cat([discrete_depth_sum, torch.zeros((discrete_depth_sum.shape[0], bin_idx_max - discrete_depth_sum.shape[1] + 1), dtype=torch.double).to(device)], axis=-1)\n",
    "        for bin_idx_iter in range(bin_idx_max+1):\n",
    "            cur_xy_tuples = torch.where(discrete_depths[img_i][0] == bin_idx_iter)\n",
    "            if len(cur_xy_tuples[0]) == 0:\n",
    "                continue\n",
    "            discrete_depth_sum[:, bin_idx_iter] += (fmaps[img_i, :, cur_xy_tuples[0], cur_xy_tuples[1]]).sum(-1)\n",
    "\n",
    "    return discrete_depth_sum\n",
    "\n",
    "\n",
    "def compute_selectivity(depth_avg_response, unit_max_bin):\n",
    "    if isinstance(depth_avg_response, np.ndarray):\n",
    "        depth_avg_response = torch.from_numpy(depth_avg_response)\n",
    "    response_max = depth_avg_response[np.arange(len(depth_avg_response)), unit_max_bin]\n",
    "    response_nonmax = (depth_avg_response.sum(1) - response_max) / (depth_avg_response.shape[1] - 1)\n",
    "    selectivity_index = (response_max - response_nonmax) / (response_max + response_nonmax + 1e-15)\n",
    "    return selectivity_index\n",
    "\n",
    "\n",
    "def get_features(model, image, layer_list, also_get_output=True, is_training=False):\n",
    "    return_list = []\n",
    "\n",
    "    def forward(image, layer_list, also_get_output):\n",
    "        x, enc_layer = model.encoder(image)\n",
    "        output, dec_layer = model.decoder(x, enc_layer)\n",
    "    \n",
    "        for layer in layer_list:\n",
    "            if layer == 'enc_output':\n",
    "                return_list.append(x)\n",
    "            elif layer.startswith('enc_'):  # e.g., 'enc_0', 'enc_1', ..., 'enc_9'\n",
    "                idx = int(layer.split('_')[1])\n",
    "                return_list.append(enc_layer[idx])\n",
    "            elif layer.startswith('dec_'):  # e.g., 'dec_0', 'dec_1', ..., 'dec_9'\n",
    "                idx = int(layer.split('_')[1])\n",
    "                return_list.append(dec_layer[idx])\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Layer {layer} is not supported.\")\n",
    "\n",
    "        if also_get_output:\n",
    "            return_list.append(output)\n",
    "\n",
    "        return return_list\n",
    "\n",
    "    if is_training:\n",
    "        return_list = forward(image, layer_list, also_get_output)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return_list = forward(image, layer_list, also_get_output)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:21.503591Z",
     "iopub.status.busy": "2025-07-19T14:07:21.502946Z",
     "iopub.status.idle": "2025-07-19T14:07:21.513673Z",
     "shell.execute_reply": "2025-07-19T14:07:21.512914Z",
     "shell.execute_reply.started": "2025-07-19T14:07:21.503566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_and_plot_selectivity(model, layers, data_loader, viz_plot=True):\n",
    "    num_units_list, num_bins_list, plt_row_list, plt_col_list, figsize_w_list, figsize_h_list = get_num_units_bins_and_nums_for_plt(\n",
    "        layers=layers\n",
    "    )\n",
    "\n",
    "    depth_cnt_list, depth_sum_list, depth_avg_list = [], [], []\n",
    "    for layer_i in range(len(layers)):\n",
    "        depth_cnt_list.append(torch.zeros(num_bins_list[layer_i], dtype=torch.double).to(device))\n",
    "        depth_sum_list.append(torch.zeros((num_units_list[layer_i], num_bins_list[layer_i]), dtype=torch.double).to(device))\n",
    "    sid_thresholds_list = [get_sid_thresholds(num_bins_list[layer_i]) for layer_i in range(len(layers))]\n",
    "\n",
    "    for images, depths in tqdm(data_loader, desc='Dissection'):\n",
    "        imgs = images.to(device)\n",
    "        features_list = get_features(model, imgs, layers, also_get_output=False)\n",
    "        for layer_i in range(len(layers)):\n",
    "            depths = depths\n",
    "            discrete_concepts = discretize_depths(depths, thresholds=sid_thresholds_list[layer_i])\n",
    "            depth_cnt_list[layer_i] = count_discrete_depth(discrete_concepts, depth_cnt_list[layer_i])\n",
    "            depth_sum_list[layer_i] = add_fmaps_on_discrete_depths(discrete_concepts, features_list[layer_i],\n",
    "                                                                   depth_sum_list[layer_i])\n",
    "\n",
    "    for layer_i in range(len(layers)):\n",
    "        depth_cnt_list[layer_i] = depth_cnt_list[layer_i].cpu().numpy()\n",
    "        depth_sum_list[layer_i] = depth_sum_list[layer_i].cpu().numpy()\n",
    "        depth_avg_list.append(depth_sum_list[layer_i] / (depth_cnt_list[layer_i] + 1e-15))\n",
    "\n",
    "    unit_max_bin_list, selectivity_index_list = [], []\n",
    "    for layer_i in range(len(layers)):\n",
    "        depth_avg = depth_avg_list[layer_i]\n",
    "        # ---- compute selectivity ----\n",
    "        depth_avg_abs = abs(depth_avg)\n",
    "        unit_max_bin = depth_avg_abs.argmax(-1)\n",
    "        selectivity_index = compute_selectivity(depth_avg_abs, unit_max_bin).numpy()\n",
    "        unit_max_bin_list.append(unit_max_bin)\n",
    "        selectivity_index_list.append(selectivity_index)\n",
    "        # --------------------------\n",
    "    \n",
    "        # ---- plot ----\n",
    "        if viz_plot:\n",
    "            sns.set_theme(style=\"darkgrid\")\n",
    "            fig, ax = plt.subplots(plt_row_list[layer_i], plt_col_list[layer_i],\n",
    "                                   figsize=(figsize_w_list[layer_i], figsize_h_list[layer_i]))\n",
    "        \n",
    "            for i in range(plt_row_list[layer_i]):\n",
    "                for j in range(plt_col_list[layer_i]):\n",
    "                    unit_i = i * plt_col_list[layer_i] + j\n",
    "                    ax[i, j].set_title(\n",
    "                        f'Unit {unit_i}, Layer {layers[layer_i]}, Selectivity: {selectivity_index[unit_i]:.04f}',\n",
    "                        fontsize=12)\n",
    "                    ax[i, j].bar(\n",
    "                        range(len(depth_cnt_list[layer_i])),\n",
    "                        depth_avg_abs[unit_i], # using abs value here (originally depth_avg)\n",
    "                        color='red', width=0.85)\n",
    "                    plt.xticks(fontsize=12)\n",
    "                    plt.yticks(fontsize=12)\n",
    "                    plt.subplots_adjust(wspace=0.15)\n",
    "            save_fig_name = f'{layers[layer_i]}-{selec_mean:.4f}.png'\n",
    "            fig.savefig(save_fig_name, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        selec_mean = selectivity_index.mean()\n",
    "        print(f'selec_mean_{layers[layer_i]}', selectivity_index.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth estimation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:26.806028Z",
     "iopub.status.busy": "2025-07-19T14:07:26.805779Z",
     "iopub.status.idle": "2025-07-19T14:07:26.810408Z",
     "shell.execute_reply": "2025-07-19T14:07:26.809541Z",
     "shell.execute_reply.started": "2025-07-19T14:07:26.806010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(outputs, depths):\n",
    "    mse = torch.mean((outputs - depths) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:27.155586Z",
     "iopub.status.busy": "2025-07-19T14:07:27.155258Z",
     "iopub.status.idle": "2025-07-19T14:07:27.164923Z",
     "shell.execute_reply": "2025-07-19T14:07:27.164251Z",
     "shell.execute_reply.started": "2025-07-19T14:07:27.155564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, train_loader, val_loader, device, epoch, num_samples=2, save=False):\n",
    "    \"\"\"Visualize predictions on training and validation data\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get batches of training and validation data\n",
    "    train_images, train_depths = next(iter(train_loader))\n",
    "    val_images, val_depths = next(iter(val_loader))\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        train_predictions = model(train_images.to(device))\n",
    "        val_predictions = model(val_images.to(device))\n",
    "    \n",
    "    # Convert tensors to numpy arrays\n",
    "    train_images = train_images.cpu().numpy()\n",
    "    train_depths = train_depths.cpu().numpy()\n",
    "    train_predictions = train_predictions.cpu().numpy()\n",
    "    \n",
    "    val_images = val_images.cpu().numpy()\n",
    "    val_depths = val_depths.cpu().numpy()\n",
    "    val_predictions = val_predictions.cpu().numpy()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2*num_samples, 3, figsize=(15, 5*2*num_samples))\n",
    "    plt.suptitle(f'Training and Validation Results - Epoch {epoch+1}', fontsize=16)\n",
    "    \n",
    "    def plot_sample(img, depth_gt, depth_pred, row_idx, title_prefix):\n",
    "        # Original image\n",
    "        img_display = np.transpose(img, (1, 2, 0))\n",
    "        img_display = (img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
    "        img_display = img_display.astype(np.uint8)\n",
    "        axes[row_idx, 0].imshow(img_display)\n",
    "        axes[row_idx, 0].set_title(f'{title_prefix} Input Image')\n",
    "        axes[row_idx, 0].axis('off')\n",
    "        \n",
    "        # Ground truth depth\n",
    "        axes[row_idx, 1].imshow(depth_gt[0], cmap='plasma')\n",
    "        axes[row_idx, 1].set_title(f'{title_prefix} Ground Truth Depth')\n",
    "        axes[row_idx, 1].axis('off')\n",
    "        \n",
    "        # Predicted depth\n",
    "        axes[row_idx, 2].imshow(depth_pred[0], cmap='plasma')\n",
    "        axes[row_idx, 2].set_title(f'{title_prefix} Predicted Depth')\n",
    "        axes[row_idx, 2].axis('off')\n",
    "    \n",
    "    # Plot training samples\n",
    "    for i in range(num_samples):\n",
    "        plot_sample(train_images[i], train_depths[i], train_predictions[i], 2*i, 'Train:')\n",
    "    \n",
    "    # Plot validation samples\n",
    "    for i in range(num_samples):\n",
    "        plot_sample(val_images[i], val_depths[i], val_predictions[i], 2*i + 1, 'Val:')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        os.makedirs('training_visualizations', exist_ok=True)\n",
    "        plt.savefig(f'training_visualizations/epoch_{epoch+1}.png')\n",
    "        # plt.close()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: NYUDepthV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:27.577751Z",
     "iopub.status.busy": "2025-07-19T14:07:27.577163Z",
     "iopub.status.idle": "2025-07-19T14:07:27.581412Z",
     "shell.execute_reply": "2025-07-19T14:07:27.580517Z",
     "shell.execute_reply.started": "2025-07-19T14:07:27.577726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 16 TODO\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_csv_path = '/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv'\n",
    "base_data_path = '/kaggle/input/nyu-depth-v2/nyu_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:27.794069Z",
     "iopub.status.busy": "2025-07-19T14:07:27.793785Z",
     "iopub.status.idle": "2025-07-19T14:07:27.800413Z",
     "shell.execute_reply": "2025-07-19T14:07:27.799631Z",
     "shell.execute_reply.started": "2025-07-19T14:07:27.794047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NYUDepthV2Dataset(Dataset):\n",
    "    def __init__(self, filenames_df, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filenames_df: DataFrame containing image and depth map paths\n",
    "            transform: Optional transform to be applied on the images\n",
    "        \"\"\"\n",
    "        self.filenames_df = filenames_df\n",
    "        self.transform = transform\n",
    "        # Define resolutions as constants from the paper\n",
    "        self.RGB_SIZE = (192, 256)  # (H, W) for input\n",
    "        self.DEPTH_SIZE = (48, 64)  # (H, W) for depth maps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and depth paths\n",
    "        img_path = self.filenames_df.iloc[idx, 0]\n",
    "        depth_path = self.filenames_df.iloc[idx, 1]\n",
    "        \n",
    "        # Load image and depth map\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        depth = Image.open(depth_path)\n",
    "        \n",
    "        # Resize to the specified resolutions\n",
    "        image = image.resize((self.RGB_SIZE[1], self.RGB_SIZE[0]), Image.BILINEAR)  # PIL uses (W, H)\n",
    "        depth = depth.resize((self.DEPTH_SIZE[1], self.DEPTH_SIZE[0]), Image.NEAREST)  # PIL uses (W, H)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        depth = torch.from_numpy(np.array(depth)).float().unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        return image, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:28.052601Z",
     "iopub.status.busy": "2025-07-19T14:07:28.051891Z",
     "iopub.status.idle": "2025-07-19T14:07:28.062463Z",
     "shell.execute_reply": "2025-07-19T14:07:28.061574Z",
     "shell.execute_reply.started": "2025-07-19T14:07:28.052573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_loaders(train_csv_path, base_data_path, batch_size, num_workers):\n",
    "    train_csv = Path(train_csv_path)\n",
    "    base_path = Path(base_data_path)\n",
    "\n",
    "    # Load the dataset paths\n",
    "    filenames_df = pd.read_csv(train_csv, header=None)\n",
    "    filenames_df[0] = filenames_df[0].map(lambda x: base_path/x)\n",
    "    filenames_df[1] = filenames_df[1].map(lambda x: base_path/x)\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split dataset into train and validation sets\n",
    "    total_size = len(filenames_df)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = int(0.1 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_df = filenames_df[:train_size]\n",
    "    val_df = filenames_df[train_size:train_size + val_size]\n",
    "    test_df = filenames_df[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Dataset splits: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        NYUDepthV2Dataset(train_df, transform), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        NYUDepthV2Dataset(val_df, transform), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        NYUDepthV2Dataset(test_df, transform), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:28.387368Z",
     "iopub.status.busy": "2025-07-19T14:07:28.386687Z",
     "iopub.status.idle": "2025-07-19T14:07:29.237321Z",
     "shell.execute_reply": "2025-07-19T14:07:29.236630Z",
     "shell.execute_reply.started": "2025-07-19T14:07:28.387339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: Train: 40550, Val: 5068, Test: 5070\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataset_loaders(\n",
    "    train_csv_path,\n",
    "    base_data_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network: METER model (XXS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:29.239083Z",
     "iopub.status.busy": "2025-07-19T14:07:29.238579Z",
     "iopub.status.idle": "2025-07-19T14:07:29.268918Z",
     "shell.execute_reply": "2025-07-19T14:07:29.268360Z",
     "shell.execute_reply.started": "2025-07-19T14:07:29.239062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   groups=depth,\n",
    "                                   padding=1,\n",
    "                                   stride=stride,\n",
    "                                   bias=bias).to(device)\n",
    "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
    "                        bias=False, device='cuda:0'),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b p h n d -> b p n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "        self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "        self.conv3 = conv_1x1_bn(dim, channel)\n",
    "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "\n",
    "        # Local representations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Global representations\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
    "        x = self.transformer(x)\n",
    "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n",
    "                      pw=self.pw)\n",
    "\n",
    "        # Fusion\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        L = [1, 1, 1]\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
    "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
    "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y0 = self.conv1(x)\n",
    "        # x = self.mv2[0](y0)\n",
    "        # y1 = self.mv2[1](x)\n",
    "        # x = self.mv2[2](y1)\n",
    "        # x = self.mv2[3](x)\n",
    "        # y2 = self.mv2[4](x)\n",
    "        # x = self.mvit[0](y2)\n",
    "        # y3 = self.mv2[5](x)\n",
    "        # x = self.mvit[1](y3)\n",
    "        # x = self.mv2[6](x)\n",
    "        # x = self.mvit[2](x)\n",
    "        # x = self.conv2(x)\n",
    "        # return x, [y0, y1, y2, y3]\n",
    "        econv1 = self.conv1(x)\n",
    "        mv0 = self.mv2[0](econv1)\n",
    "\n",
    "        mv1 = self.mv2[1](mv0)\n",
    "        mv2 = self.mv2[2](mv1)\n",
    "        mv3 = self.mv2[3](mv2)\n",
    "\n",
    "        mv4 = self.mv2[4](mv3)\n",
    "        meter0 = self.mvit[0](mv4)\n",
    "\n",
    "        mv5 = self.mv2[5](meter0)\n",
    "        meter1 = self.mvit[1](mv5)\n",
    "\n",
    "        mv6 = self.mv2[6](meter1)\n",
    "        meter2 = self.mvit[2](mv6)\n",
    "        x = self.conv2(meter2)\n",
    "        return x, [econv1, mv1, mv2, mv3, mv4, meter0, mv5, meter1, mv6, meter2]\n",
    "\n",
    "def mobilevit_xxs():\n",
    "    enc_type = 'xxs'\n",
    "    dims = [64, 80, 96]\n",
    "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]\n",
    "    return MobileViT((RGB_img_res[1], RGB_img_res[2]), dims, channels, expansion=2)\n",
    "\n",
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.flag = flag\n",
    "        self.name = name\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        x = self.end_up_layer(x)\n",
    "        return x\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(160, 64, kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(64, 32, flag=True, sep_conv_filters=96, name='up1', device=device)\n",
    "        self.ups_block_2 = UpSample_layer(32, 16, flag=False, sep_conv_filters=64, name='up2', device=device)\n",
    "        self.ups_block_3 = UpSample_layer(16, 8, flag=False, sep_conv_filters=32, name='up3', device=device)\n",
    "        self.conv2d_out = nn.Conv2d(8, 1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        dconv0 = self.conv2d_in(x)\n",
    "        up1 = self.ups_block_1(dconv0, enc_layer_list[7]) #enc_layer_list[3]\n",
    "        up2 = self.ups_block_2(up1, enc_layer_list[5]) #enc_layer_list[2]\n",
    "        up3 = self.ups_block_3(up2, enc_layer_list[2]) #enc_layer_list[1]\n",
    "        x = self.conv2d_out(up3)\n",
    "        return x, [dconv0, up1, up2, up3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:31.150080Z",
     "iopub.status.busy": "2025-07-19T14:07:31.149804Z",
     "iopub.status.idle": "2025-07-19T14:07:31.154600Z",
     "shell.execute_reply": "2025-07-19T14:07:31.153942Z",
     "shell.execute_reply.started": "2025-07-19T14:07:31.150061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class build_METER_model(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(build_METER_model, self).__init__()\n",
    "        self.encoder = mobilevit_xxs()\n",
    "        self.decoder = decoder(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, enc_layer = self.encoder(x)\n",
    "        x, dec_layer = self.decoder(x, enc_layer)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Baseline loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:31.754391Z",
     "iopub.status.busy": "2025-07-19T14:07:31.754088Z",
     "iopub.status.idle": "2025-07-19T14:07:31.766855Z",
     "shell.execute_reply": "2025-07-19T14:07:31.766216Z",
     "shell.execute_reply.started": "2025-07-19T14:07:31.754369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([np.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n",
    "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
    "    if val_range is None:\n",
    "        if torch.max(img1) > 128:\n",
    "            max_val = 255\n",
    "        else:\n",
    "            max_val = 1\n",
    "\n",
    "        if torch.min(img1) < -0.5:\n",
    "            min_val = -1\n",
    "        else:\n",
    "            min_val = 0\n",
    "        L = max_val - min_val\n",
    "    else:\n",
    "        L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.edge_conv(x)\n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:33.079879Z",
     "iopub.status.busy": "2025-07-19T14:07:33.079603Z",
     "iopub.status.idle": "2025-07-19T14:07:33.087252Z",
     "shell.execute_reply": "2025-07-19T14:07:33.086650Z",
     "shell.execute_reply.started": "2025-07-19T14:07:33.079861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class balanced_loss_function(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(balanced_loss_function, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "        self.get_gradient = Sobel().to(device)\n",
    "        self.device = device\n",
    "        self.lambda_1 = 0.5\n",
    "        self.lambda_2 = 100\n",
    "        self.lambda_3 = 100\n",
    "\n",
    "    def forward(self, output, depth):\n",
    "        with torch.no_grad():\n",
    "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
    "\n",
    "        depth_grad = self.get_gradient(depth)\n",
    "        output_grad = self.get_gradient(output)\n",
    "\n",
    "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "\n",
    "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
    "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
    "\n",
    "        loss_depth = torch.abs(output - depth).mean()\n",
    "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
    "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
    "        loss_normal = self.lambda_2 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
    "\n",
    "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * self.lambda_3\n",
    "\n",
    "        loss_grad = (loss_dx + loss_dy) / self.lambda_1\n",
    "\n",
    "        loss = loss_depth + loss_ssim + loss_normal + loss_grad\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron selectivity regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:45:20.481759Z",
     "iopub.status.busy": "2025-07-19T13:45:20.481208Z",
     "iopub.status.idle": "2025-07-19T13:45:20.488241Z",
     "shell.execute_reply": "2025-07-19T13:45:20.487618Z",
     "shell.execute_reply.started": "2025-07-19T13:45:20.481734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loss_with_regularizer(layers, model, imgs, depths, sid_thresholds_list, device, criterion):\n",
    "    features_list = get_features(model, imgs, layers, also_get_output=True, is_training=True)\n",
    "    preds = features_list[-1]\n",
    "    baseline_loss = criterion(preds, depths)\n",
    "\n",
    "    # ---- selectivity regularizer ----\n",
    "    selec_list = []\n",
    "    for layer_i in range(len(layers)):\n",
    "        discret_depths = discretize_depths(depths, thresholds=sid_thresholds_list[layer_i])\n",
    "        concept_cnt = count_discrete_depth(discret_depths, torch.zeros(len(sid_thresholds_list[layer_i])-1, dtype=torch.double).to(device))\n",
    "        features_list[layer_i] = torch.abs(features_list[layer_i])\n",
    "        concept_sum = add_fmaps_on_discrete_depths(discret_depths, features_list[layer_i], torch.zeros((features_list[layer_i].shape[1], len(sid_thresholds_list[layer_i])-1), dtype=torch.double).to(device))\n",
    "        concept_avg = concept_sum / (concept_cnt + 1e-15)\n",
    "\n",
    "        num_units, num_bins = concept_avg.shape[0], concept_avg.shape[1]\n",
    "        assert num_units >= num_bins\n",
    "        unit_class = (torch.arange(num_units)//(num_units/num_bins)).to(concept_avg.device).long()\n",
    "        layer_selec = compute_selectivity(concept_avg, unit_class)\n",
    "        concept_cnt_iszero = (unit_class == torch.nonzero(concept_cnt==0)).sum(0, dtype=torch.bool)\n",
    "        layer_selec[concept_cnt_iszero] = 0   # If d_k is absent in a batch, the unit k will be simply disregarded from the loss computation\n",
    "        layer_selec = layer_selec.mean()\n",
    "        selec_list.append(layer_selec)\n",
    "    selectivity = sum(selec_list)\n",
    "    return baseline_loss, selectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:45:21.285287Z",
     "iopub.status.busy": "2025-07-19T13:45:21.285001Z",
     "iopub.status.idle": "2025-07-19T13:45:21.318867Z",
     "shell.execute_reply": "2025-07-19T13:45:21.318314Z",
     "shell.execute_reply.started": "2025-07-19T13:45:21.285266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "model = build_METER_model(device).to(device)\n",
    "criterion = balanced_loss_function(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "# For train with neuron selectivity\n",
    "imde_alpha = -0.1\n",
    "sid_thresholds_list = []\n",
    "cfg_layers = [\"enc_3\"] # TODO use last up-sampling of decoder (D) and concat conv of upsampling of encoder (MFF). tdlr: layers closest to depth output \n",
    "num_bins_list = get_num_units_bins_and_nums_for_plt(cfg_layers)[1]\n",
    "for layer_i in range(len(cfg_layers)):\n",
    "    sid_thresholds_list.append(get_sid_thresholds(num_bins_list[layer_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:45:22.329843Z",
     "iopub.status.busy": "2025-07-19T13:45:22.329296Z",
     "iopub.status.idle": "2025-07-19T13:45:22.336851Z",
     "shell.execute_reply": "2025-07-19T13:45:22.336152Z",
     "shell.execute_reply.started": "2025-07-19T13:45:22.329817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, selectivity_sum, baseline_loss_sum = 0, 0, 0\n",
    "    #TODO Batch sampling\n",
    "    # for batch_i, batch in enumerate(tqdm(train_loader, desc='Training')):\n",
    "    #     images = batch['image'].to(device)\n",
    "    #     depths = batch['depth'].to(device)\n",
    "    for images, depths in tqdm(train_loader, desc='Training'):\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "\n",
    "        loss = None\n",
    "        if neuron_selective:\n",
    "            baseline_loss, selectivity = get_loss_with_regularizer(cfg_layers, model, images, depths, sid_thresholds_list, device, criterion)\n",
    "            loss = baseline_loss + imde_alpha * selectivity\n",
    "            selectivity_sum += selectivity.item()\n",
    "            baseline_loss_sum += baseline_loss.item()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, depths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(train_loader), selectivity_sum/len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    rmse = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, depths in tqdm(val_loader, desc='Validating'):\n",
    "            images = images.to(device)\n",
    "            depths = depths.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = None\n",
    "            if neuron_selective:\n",
    "                baseline_loss, selectivity = get_loss_with_regularizer(cfg_layers, model, images, depths, sid_thresholds_list, device, criterion)\n",
    "                loss = baseline_loss + imde_alpha * selectivity\n",
    "            else:\n",
    "                loss = criterion(outputs, depths)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            rmse += compute_rmse(outputs, depths)\n",
    "\n",
    "    return total_loss / len(val_loader), rmse / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T20:22:22.294565Z",
     "iopub.status.busy": "2025-07-17T20:22:22.293948Z",
     "iopub.status.idle": "2025-07-17T20:22:22.298579Z",
     "shell.execute_reply": "2025-07-17T20:22:22.297717Z",
     "shell.execute_reply.started": "2025-07-17T20:22:22.294543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses, val_losses, selectivities = [], [], []\n",
    "rmse_values = []\n",
    "print(f'***** Neuron selectivite: {neuron_selective} *****')\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, selectivity = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    selectivities.append(selectivity)\n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "    if neuron_selective:\n",
    "        print(f'Selectivity: {selectivity:.4f}')\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, rmse = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    rmse_values.append(rmse)\n",
    "    print(f'Validation Loss: {val_loss:.4f} RMSE: {rmse:.4f}')\n",
    "    # TODO selectivity mean\n",
    "    # Visualize predictions on training data\n",
    "    # if epoch % 10 == 0:\n",
    "    # Visualize predictions on training and validation data\n",
    "    visualize_predictions(model, train_loader, val_loader, device, epoch, save=True)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'selectivity': selectivity,\n",
    "            'val_loss': val_loss,\n",
    "            'rmse': rmse,\n",
    "        }, 'best_meter_model.pth')\n",
    "    \n",
    "    print('-' * 50)\n",
    "    clear_gpu_memory()\n",
    "# Save logs\n",
    "np.save('train_losses.npy', train_losses)\n",
    "np.save('val_losses.npy', val_losses)\n",
    "np.save('selectivities.npy', selectivities)\n",
    "np.save('rmse_values.npy', rmse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T20:26:30.699460Z",
     "iopub.status.busy": "2025-07-17T20:26:30.699180Z",
     "iopub.status.idle": "2025-07-17T20:26:30.704954Z",
     "shell.execute_reply": "2025-07-17T20:26:30.704374Z",
     "shell.execute_reply.started": "2025-07-17T20:26:30.699438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load logs\n",
    "# train_losses = np.load('train_losses.npy')\n",
    "# val_losses = np.load('val_losses.npy')\n",
    "# selectivities = np.load('selectivities.npy')\n",
    "# rmse_values = np.load('rmse_values.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T20:26:32.847047Z",
     "iopub.status.busy": "2025-07-17T20:26:32.846346Z",
     "iopub.status.idle": "2025-07-17T20:26:33.202467Z",
     "shell.execute_reply": "2025-07-17T20:26:33.201665Z",
     "shell.execute_reply.started": "2025-07-17T20:26:32.847019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot train & val losses\n",
    "plt_epochs = range(1, num_epochs+1)\n",
    "plt.plot(plt_epochs, train_losses, label='Training Loss')\n",
    "plt.plot(plt_epochs, val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(np.arange(0, num_epochs+1, 2))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'train_val_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T20:26:37.428316Z",
     "iopub.status.busy": "2025-07-17T20:26:37.428036Z",
     "iopub.status.idle": "2025-07-17T20:26:37.754420Z",
     "shell.execute_reply": "2025-07-17T20:26:37.753668Z",
     "shell.execute_reply.started": "2025-07-17T20:26:37.428297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot RMSE\n",
    "plt_epochs = range(1, num_epochs+1)\n",
    "plt.plot(plt_epochs, rmse_values, label='RMSE')\n",
    "plt.title('Root mean squared error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(np.arange(0, num_epochs+1, 2))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'rmse.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T20:28:01.073659Z",
     "iopub.status.busy": "2025-07-17T20:28:01.073080Z",
     "iopub.status.idle": "2025-07-17T20:28:01.424011Z",
     "shell.execute_reply": "2025-07-17T20:28:01.423200Z",
     "shell.execute_reply.started": "2025-07-17T20:28:01.073636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot selectivity\n",
    "plt_epochs = range(1, num_epochs+1)\n",
    "plt.plot(plt_epochs, selectivities, label='Selectivity')\n",
    "plt.title('Neuron depth selectivity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Selectivity')\n",
    "plt.xticks(np.arange(0, num_epochs+1, 2))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'selectivity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:38:50.717001Z",
     "iopub.status.busy": "2025-07-19T13:38:50.716189Z",
     "iopub.status.idle": "2025-07-19T13:38:58.034104Z",
     "shell.execute_reply": "2025-07-19T13:38:58.033361Z",
     "shell.execute_reply.started": "2025-07-19T13:38:50.716971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Processing file 11r80OJm4GuOCHj_yQiCgXbxeul66XyCZ best_meter_model.pth\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11r80OJm4GuOCHj_yQiCgXbxeul66XyCZ\n",
      "To: /kaggle/working/model/best_meter_model.pth\n",
      "100%|| 8.81M/8.81M [00:00<00:00, 103MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1uz1VgAqYP17idqKvXAKiFTJpoPXDYQ5m -O model\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:37.388212Z",
     "iopub.status.busy": "2025-07-19T14:07:37.387966Z",
     "iopub.status.idle": "2025-07-19T14:07:37.545544Z",
     "shell.execute_reply": "2025-07-19T14:07:37.544779Z",
     "shell.execute_reply.started": "2025-07-19T14:07:37.388195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_METER_model(\n",
       "  (encoder): MobileViT(\n",
       "    (conv1): Sequential(\n",
       "      (0): SeparableConv2d(\n",
       "        (depthwise): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (pointwise): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (mv2): ModuleList(\n",
       "      (0): MV2Block(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MV2Block(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x MV2Block(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): MV2Block(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): MV2Block(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): MV2Block(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mvit): ModuleList(\n",
       "      (0): MobileViTBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): SeparableConv2d(\n",
       "            (depthwise): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (pointwise): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (to_qkv): Linear(in_features=64, out_features=96, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (conv4): Sequential(\n",
       "          (0): SeparableConv2d(\n",
       "            (depthwise): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (pointwise): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): MobileViTBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): SeparableConv2d(\n",
       "            (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (to_qkv): Linear(in_features=80, out_features=96, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=80, out_features=320, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=320, out_features=80, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (conv4): Sequential(\n",
       "          (0): SeparableConv2d(\n",
       "            (depthwise): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (2): MobileViTBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): SeparableConv2d(\n",
       "            (depthwise): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (pointwise): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (to_qkv): Linear(in_features=96, out_features=96, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=32, out_features=96, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Dropout(p=0.0, inplace=False)\n",
       "                    (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (4): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (conv4): Sequential(\n",
       "          (0): SeparableConv2d(\n",
       "            (depthwise): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (pointwise): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (decoder): decoder(\n",
       "    (conv2d_in): Conv2d(160, 64, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "    (ups_block_1): UpSample_layer(\n",
       "      (conv2d_transpose): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (end_up_layer): Sequential(\n",
       "        (0): SeparableConv2d(\n",
       "          (depthwise): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (ups_block_2): UpSample_layer(\n",
       "      (conv2d_transpose): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (end_up_layer): Sequential(\n",
       "        (0): SeparableConv2d(\n",
       "          (depthwise): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (pointwise): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (ups_block_3): UpSample_layer(\n",
       "      (conv2d_transpose): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "      (end_up_layer): Sequential(\n",
       "        (0): SeparableConv2d(\n",
       "          (depthwise): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (pointwise): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv2d_out): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_METER_model(device).to(device)\n",
    "# model.load_state_dict(torch.load('best_meter_model.pth')['model_state_dict'])\n",
    "model.load_state_dict(torch.load('model/best_meter_model.pth')['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:57:07.065784Z",
     "iopub.status.busy": "2025-07-19T13:57:07.065218Z",
     "iopub.status.idle": "2025-07-19T13:58:18.525213Z",
     "shell.execute_reply": "2025-07-19T13:58:18.524452Z",
     "shell.execute_reply.started": "2025-07-19T13:57:07.065759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dissection: 100%|| 80/80 [01:11<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selec_mean_dec_0 0.6296107139659783\n",
      "selec_mean_dec_1 0.5464890521079142\n",
      "selec_mean_dec_2 0.5636882458743551\n",
      "selec_mean_dec_3 0.5411996342563707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layers_to_compute_selectivity = [\"dec_0\", \"dec_1\", \"dec_2\",\"dec_3\"]\n",
    "compute_and_plot_selectivity(model, layers=layers_to_compute_selectivity, data_loader=val_loader, viz_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T14:07:46.503250Z",
     "iopub.status.busy": "2025-07-19T14:07:46.502531Z",
     "iopub.status.idle": "2025-07-19T14:12:22.003035Z",
     "shell.execute_reply": "2025-07-19T14:12:22.001986Z",
     "shell.execute_reply.started": "2025-07-19T14:07:46.503225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dissection: 100%|| 80/80 [04:35<00:00,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selec_mean_enc_0 0.5274475860277759\n",
      "selec_mean_enc_1 0.7420573937896534\n",
      "selec_mean_enc_2 0.7496523388994532\n",
      "selec_mean_enc_3 0.756596380977486\n",
      "selec_mean_enc_4 0.752473672912069\n",
      "selec_mean_enc_5 0.5556769536699088\n",
      "selec_mean_enc_6 0.7097257585106098\n",
      "selec_mean_enc_7 0.5739998923044067\n",
      "selec_mean_enc_8 0.7184245854225952\n",
      "selec_mean_enc_9 0.6493387697812792\n",
      "selec_mean_enc_output 0.6655900725930121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layers_to_compute_selectivity = [\"enc_0\", \"enc_1\", \"enc_2\",\"enc_3\",\"enc_4\", \"enc_5\", \"enc_6\",\"enc_7\",\"enc_8\", \"enc_9\", \"enc_output\"]\n",
    "# layers_to_compute_selectivity = [\"enc_output\"]\n",
    "compute_and_plot_selectivity(model, layers=layers_to_compute_selectivity, data_loader=val_loader, viz_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1198025,
     "sourceId": 2002504,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
